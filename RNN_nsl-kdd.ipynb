{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [300/1000], Loss: 0.0594\n",
      "Epoch [1/4], Step [600/1000], Loss: 0.0375\n",
      "Epoch [1/4], Step [900/1000], Loss: 0.0122\n",
      "Epoch [2/4], Step [300/1000], Loss: 0.0190\n",
      "Epoch [2/4], Step [600/1000], Loss: 0.0837\n",
      "Epoch [2/4], Step [900/1000], Loss: 0.0076\n",
      "Epoch [3/4], Step [300/1000], Loss: 0.0251\n",
      "Epoch [3/4], Step [600/1000], Loss: 0.0793\n",
      "Epoch [3/4], Step [900/1000], Loss: 0.0078\n",
      "Epoch [4/4], Step [300/1000], Loss: 0.0285\n",
      "Epoch [4/4], Step [600/1000], Loss: 0.0223\n",
      "Epoch [4/4], Step [900/1000], Loss: 0.0063\n",
      "Validation Accuracy of the model : 98.98069498069498 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 1\n",
    "input_size = 41\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 5\n",
    "batch_size = 100\n",
    "num_epochs = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# NSL-KDD datasets\n",
    "df = pd.read_csv('NSL-KDD/KDDTrain+.txt', sep=',')\n",
    "df.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "       'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "       'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
    "       'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "       'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "       'dst_host_srv_rerror_rate', 'labels', 'difficulty']\n",
    "\n",
    "# Convert categorial feature into numeric\n",
    "protocols = df.protocol_type.value_counts()\n",
    "protocols_map = {}\n",
    "for i, (prop, count) in enumerate(protocols.items()):\n",
    "    protocols_map[prop] = i\n",
    "df.protocol_type = df.protocol_type.map(protocols_map)\n",
    "\n",
    "# Convert categorial feature into numeric\n",
    "services = df.service.value_counts()\n",
    "service_map = {}\n",
    "for i, (ser, count) in enumerate(services.items()):\n",
    "    service_map[ser] = i\n",
    "df.service = df.service.map(service_map)\n",
    "\n",
    "# Convert categorial feature into numeric\n",
    "flag_map = {}\n",
    "flags = df.flag.value_counts()\n",
    "for i, (flag, count) in enumerate(flags.items()):\n",
    "    flag_map[flag] = i\n",
    "df.flag = df.flag.map(flag_map)\n",
    "\n",
    "# Rename every attack label: 0=normal, 1=DoS, 2=Probe, 3=R2L and 4=U2R.\n",
    "df['targets'] = df.labels.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "\n",
    "# Convert into train and test datasets\n",
    "train_data, val_data = df[:100000], df[100000:125900]\n",
    "\n",
    "# Seperate features and target\n",
    "train_data_x = train_data.drop(['labels', 'difficulty', 'targets'], axis=1)\n",
    "train_data_y = train_data['targets']\n",
    "val_data_x = val_data.drop(['labels', 'difficulty', 'targets'], axis=1)\n",
    "val_data_y = val_data['targets']\n",
    "\n",
    "# apply standardscaler into datasets\n",
    "scaler = StandardScaler()\n",
    "train_data_x = scaler.fit_transform(train_data_x)\n",
    "val_data_x = scaler.fit_transform(val_data_x)\n",
    "\n",
    "# train_data = TensorDataset(torch.tensor(train_data_x.values.astype(np.float32)).type(torch.LongTensor), torch.tensor(train_data_y.values.astype(np.float32)).type(torch.LongTensor))\n",
    "# val_data = TensorDataset(torch.from_numpy(val_data_x.values.astype(np.float32)).type(torch.LongTensor), torch.from_numpy(val_data_y.values.astype(np.float32)).type(torch.LongTensor))\n",
    "\n",
    "# convert into tensor datasets\n",
    "train_data = TensorDataset(torch.tensor(train_data_x), torch.tensor(train_data_y.values.astype(np.float32)).type(torch.LongTensor))\n",
    "val_data = TensorDataset(torch.tensor(val_data_x), torch.from_numpy(val_data_y.values.astype(np.float32)).type(torch.LongTensor))\n",
    "\n",
    "# dataloader\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_data_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "\n",
    "        out, _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (input_data, labels) in enumerate(train_data_loader):\n",
    "        input_data = input_data.reshape(-1, sequence_length, input_size).to(device)\n",
    "        input_data = input_data.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input_data, labels in valid_data_loader:\n",
    "        input_data = input_data.reshape(-1, sequence_length, input_size).to(device)\n",
    "        input_data = input_data.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Validation Accuracy of the model : {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
